{"cells":[{"cell_type":"markdown","source":["# Machine Learning With Spark ML\nIn this lab assignment, you will complete a project by going through the following steps:\n1. Get the data.\n2. Discover the data to gain insights.\n3. Prepare the data for Machine Learning algorithms.\n4. Select a model and train it.\n5. Fine-tune your model.\n6. Present your solution.\n\nAs a dataset, we use the California Housing Prices dataset from the StatLib repository. This dataset was based on data from the 1990 California census. The dataset has the following columns\n1. `longitude`: a measure of how far west a house is (a higher value is farther west)\n2. `latitude`: a measure of how far north a house is (a higher value is farther north)\n3. `housing_,median_age`: median age of a house within a block (a lower number is a newer building)\n4. `total_rooms`: total number of rooms within a block\n5. `total_bedrooms`: total number of bedrooms within a block\n6. `population`: total number of people residing within a block\n7. `households`: total number of households, a group of people residing within a home unit, for a block\n8. `median_income`: median income for households within a block of houses\n9. `median_house_value`: median house value for households within a block\n10. `ocean_proximity`: location of the house w.r.t ocean/sea\n\n---\n# 1. Get the data\nLet's start the lab by loading the dataset. The can find the dataset at `data/housing.csv`. To infer column types automatically, when you are reading the file, you need to set `inferSchema` to true. Moreover enable the `header` option to read the columns' name from the file."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d520a980-c08f-467c-9903-924e3b00d9c2"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nval housing = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"True\").csv(\"/FileStore/tables/lab1/housing.csv\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffbe6107-aef8-4c8a-8c87-94ea08b3dfc1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"housing","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"longitude","type":"double","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"housing_median_age","type":"double","nullable":true,"metadata":{}},{"name":"total_rooms","type":"double","nullable":true,"metadata":{}},{"name":"total_bedrooms","type":"double","nullable":true,"metadata":{}},{"name":"population","type":"double","nullable":true,"metadata":{}},{"name":"households","type":"double","nullable":true,"metadata":{}},{"name":"median_income","type":"double","nullable":true,"metadata":{}},{"name":"median_house_value","type":"double","nullable":true,"metadata":{}},{"name":"ocean_proximity","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">housing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 8 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">housing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 8 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["---\n# 2. Discover the data to gain insights\nNow it is time to take a look at the data. In this step we are going to take a look at the data a few different ways:\n* See the schema and dimension of the dataset\n* Look at the data itself\n* Statistical summary of the attributes\n* Breakdown of the data by the categorical attribute variable\n* Find the correlation among different attributes\n* Make new attributes by combining existing attributes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"348ae06f-7d1b-496d-85bc-ee0683dd07a4"}}},{"cell_type":"markdown","source":["## 2.1. Schema and dimension\nPrint the schema of the dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf9427b6-d557-48d4-b7b9-f3254d5e3b0c"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\nhousing.schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9215121d-abae-429f-8ab9-a465ad3cdf0e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res1: org.apache.spark.sql.types.StructType = StructType(StructField(longitude,DoubleType,true), StructField(latitude,DoubleType,true), StructField(housing_median_age,DoubleType,true), StructField(total_rooms,DoubleType,true), StructField(total_bedrooms,DoubleType,true), StructField(population,DoubleType,true), StructField(households,DoubleType,true), StructField(median_income,DoubleType,true), StructField(median_house_value,DoubleType,true), StructField(ocean_proximity,StringType,true))\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res1: org.apache.spark.sql.types.StructType = StructType(StructField(longitude,DoubleType,true), StructField(latitude,DoubleType,true), StructField(housing_median_age,DoubleType,true), StructField(total_rooms,DoubleType,true), StructField(total_bedrooms,DoubleType,true), StructField(population,DoubleType,true), StructField(households,DoubleType,true), StructField(median_income,DoubleType,true), StructField(median_house_value,DoubleType,true), StructField(ocean_proximity,StringType,true))\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Print the number of records in the dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4f3a752-675c-4097-bfda-4f553616e076"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nhousing.count"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c497392-ce2c-4ab9-83ee-29bc295965b7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res2: Long = 20640\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res2: Long = 20640\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2.2. Look at the data\nPrint the first five records of the dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"513aedeb-ad82-4752-b4da-d4ba8c73bec0"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nhousing.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48f8f074-6ef6-49f7-9e2b-d1e925b4045e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\nlongitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Print the number of records with population more than 10000."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"537a829d-d6cf-496d-b74c-c81355130f73"}}},{"cell_type":"code","source":["%scala\n\n// TODO: Replace <FILL IN> with appropriate code\n\nhousing.filter($\"population\" >=10000).count"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0784e2c6-0ec7-4694-b9e6-f1bb20c65e76"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res4: Long = 23\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res4: Long = 23\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2.3. Statistical summary\nPrint a summary of the table statistics for the attributes `housing_median_age`, `total_rooms`, `median_house_value`, and `population`. You can use the `describe` command."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64c2a808-3771-46a5-a33e-c30dbd8b51e3"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nhousing.select(\"housing_median_age\", \"total_rooms\", \"median_house_value\", \"population\").describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"016de718-ed2f-44ad-b519-6604301f4b73"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+------------------+------------------+------------------+------------------+\n|summary|housing_median_age|       total_rooms|median_house_value|        population|\n+-------+------------------+------------------+------------------+------------------+\n|  count|             20640|             20640|             20640|             20640|\n|   mean|28.639486434108527|2635.7630813953488|206855.81690891474|1425.4767441860465|\n| stddev| 12.58555761211163|2181.6152515827944|115395.61587441359|  1132.46212176534|\n|    min|               1.0|               2.0|           14999.0|               3.0|\n|    max|              52.0|           39320.0|          500001.0|           35682.0|\n+-------+------------------+------------------+------------------+------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------------------+------------------+------------------+------------------+\nsummary|housing_median_age|       total_rooms|median_house_value|        population|\n+-------+------------------+------------------+------------------+------------------+\n  count|             20640|             20640|             20640|             20640|\n   mean|28.639486434108527|2635.7630813953488|206855.81690891474|1425.4767441860465|\n stddev| 12.58555761211163|2181.6152515827944|115395.61587441359|  1132.46212176534|\n    min|               1.0|               2.0|           14999.0|               3.0|\n    max|              52.0|           39320.0|          500001.0|           35682.0|\n+-------+------------------+------------------+------------------+------------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Print the maximum age (`housing_median_age`), the minimum number of rooms (`total_rooms`), and the average of house values (`median_house_value`)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"462e423a-f264-4394-8b5e-2046f357d8db"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.sql.functions._\n\nhousing.agg(max(\"housing_median_age\"), min(\"total_rooms\"), avg(\"median_house_value\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f74a432d-7250-44a4-87f2-8a921b905446"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------------+----------------+-----------------------+\n|max(housing_median_age)|min(total_rooms)|avg(median_house_value)|\n+-----------------------+----------------+-----------------------+\n|                   52.0|             2.0|     206855.81690891474|\n+-----------------------+----------------+-----------------------+\n\nimport org.apache.spark.sql.functions._\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------+----------------+-----------------------+\nmax(housing_median_age)|min(total_rooms)|avg(median_house_value)|\n+-----------------------+----------------+-----------------------+\n                   52.0|             2.0|     206855.81690891474|\n+-----------------------+----------------+-----------------------+\n\nimport org.apache.spark.sql.functions._\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2.4. Breakdown the data by categorical data\nPrint the number of houses in different areas (`ocean_proximity`), and sort them in descending order."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f069ab7-edae-43e5-97a2-e22320f39a58"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nhousing.groupBy(\"ocean_proximity\").count().sort(desc(\"count\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ebfce45-473e-4bd8-a912-87b79797d676"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------------+-----+\n|ocean_proximity|count|\n+---------------+-----+\n|      &lt;1H OCEAN| 9136|\n|         INLAND| 6551|\n|     NEAR OCEAN| 2658|\n|       NEAR BAY| 2290|\n|         ISLAND|    5|\n+---------------+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+-----+\nocean_proximity|count|\n+---------------+-----+\n      &lt;1H OCEAN| 9136|\n         INLAND| 6551|\n     NEAR OCEAN| 2658|\n       NEAR BAY| 2290|\n         ISLAND|    5|\n+---------------+-----+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Print the average value of the houses (`median_house_value`) in different areas (`ocean_proximity`), and call the new column `avg_value` when print it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f00fd2eb-d5a7-49f3-8493-f7068cc75714"}}},{"cell_type":"code","source":["%scala\nhousing.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfc08c05-d838-462d-bbe6-22c0c8894f11"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n|  -122.25|   37.85|              52.0|      919.0|         213.0|     413.0|     193.0|       4.0368|          269700.0|       NEAR BAY|\n|  -122.25|   37.84|              52.0|     2535.0|         489.0|    1094.0|     514.0|       3.6591|          299200.0|       NEAR BAY|\n|  -122.25|   37.84|              52.0|     3104.0|         687.0|    1157.0|     647.0|         3.12|          241400.0|       NEAR BAY|\n|  -122.26|   37.84|              42.0|     2555.0|         665.0|    1206.0|     595.0|       2.0804|          226700.0|       NEAR BAY|\n|  -122.25|   37.84|              52.0|     3549.0|         707.0|    1551.0|     714.0|       3.6912|          261100.0|       NEAR BAY|\n|  -122.26|   37.85|              52.0|     2202.0|         434.0|     910.0|     402.0|       3.2031|          281500.0|       NEAR BAY|\n|  -122.26|   37.85|              52.0|     3503.0|         752.0|    1504.0|     734.0|       3.2705|          241800.0|       NEAR BAY|\n|  -122.26|   37.85|              52.0|     2491.0|         474.0|    1098.0|     468.0|        3.075|          213500.0|       NEAR BAY|\n|  -122.26|   37.84|              52.0|      696.0|         191.0|     345.0|     174.0|       2.6736|          191300.0|       NEAR BAY|\n|  -122.26|   37.85|              52.0|     2643.0|         626.0|    1212.0|     620.0|       1.9167|          159200.0|       NEAR BAY|\n|  -122.26|   37.85|              50.0|     1120.0|         283.0|     697.0|     264.0|        2.125|          140000.0|       NEAR BAY|\n|  -122.27|   37.85|              52.0|     1966.0|         347.0|     793.0|     331.0|        2.775|          152500.0|       NEAR BAY|\n|  -122.27|   37.85|              52.0|     1228.0|         293.0|     648.0|     303.0|       2.1202|          155500.0|       NEAR BAY|\n|  -122.26|   37.84|              50.0|     2239.0|         455.0|     990.0|     419.0|       1.9911|          158700.0|       NEAR BAY|\n|  -122.27|   37.84|              52.0|     1503.0|         298.0|     690.0|     275.0|       2.6033|          162900.0|       NEAR BAY|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\nlongitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n  -122.25|   37.85|              52.0|      919.0|         213.0|     413.0|     193.0|       4.0368|          269700.0|       NEAR BAY|\n  -122.25|   37.84|              52.0|     2535.0|         489.0|    1094.0|     514.0|       3.6591|          299200.0|       NEAR BAY|\n  -122.25|   37.84|              52.0|     3104.0|         687.0|    1157.0|     647.0|         3.12|          241400.0|       NEAR BAY|\n  -122.26|   37.84|              42.0|     2555.0|         665.0|    1206.0|     595.0|       2.0804|          226700.0|       NEAR BAY|\n  -122.25|   37.84|              52.0|     3549.0|         707.0|    1551.0|     714.0|       3.6912|          261100.0|       NEAR BAY|\n  -122.26|   37.85|              52.0|     2202.0|         434.0|     910.0|     402.0|       3.2031|          281500.0|       NEAR BAY|\n  -122.26|   37.85|              52.0|     3503.0|         752.0|    1504.0|     734.0|       3.2705|          241800.0|       NEAR BAY|\n  -122.26|   37.85|              52.0|     2491.0|         474.0|    1098.0|     468.0|        3.075|          213500.0|       NEAR BAY|\n  -122.26|   37.84|              52.0|      696.0|         191.0|     345.0|     174.0|       2.6736|          191300.0|       NEAR BAY|\n  -122.26|   37.85|              52.0|     2643.0|         626.0|    1212.0|     620.0|       1.9167|          159200.0|       NEAR BAY|\n  -122.26|   37.85|              50.0|     1120.0|         283.0|     697.0|     264.0|        2.125|          140000.0|       NEAR BAY|\n  -122.27|   37.85|              52.0|     1966.0|         347.0|     793.0|     331.0|        2.775|          152500.0|       NEAR BAY|\n  -122.27|   37.85|              52.0|     1228.0|         293.0|     648.0|     303.0|       2.1202|          155500.0|       NEAR BAY|\n  -122.26|   37.84|              50.0|     2239.0|         455.0|     990.0|     419.0|       1.9911|          158700.0|       NEAR BAY|\n  -122.27|   37.84|              52.0|     1503.0|         298.0|     690.0|     275.0|       2.6033|          162900.0|       NEAR BAY|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\nimport org.apache.spark.sql.types._\n//val df2 = housing.withColumn(\"avgtmp\", $\"median_house_value\".cast(IntegerType))\n//    .drop(\"median_house_value\")\n //   .withColumnRenamed(\"avgtmp\", \"median_house_value\")\n\nhousing.groupBy(\"ocean_proximity\").avg(\"median_house_value\").withColumnRenamed(\"avg(median_house_value)\", \"avg_value\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efd19c29-465f-435d-9612-58bf7f0fa2d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------------+------------------+\n|ocean_proximity|         avg_value|\n+---------------+------------------+\n|         ISLAND|          380440.0|\n|     NEAR OCEAN|249433.97742663656|\n|       NEAR BAY|259212.31179039303|\n|      &lt;1H OCEAN|240084.28546409807|\n|         INLAND|124805.39200122119|\n+---------------+------------------+\n\nimport org.apache.spark.sql.types._\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+------------------+\nocean_proximity|         avg_value|\n+---------------+------------------+\n         ISLAND|          380440.0|\n     NEAR OCEAN|249433.97742663656|\n       NEAR BAY|259212.31179039303|\n      &lt;1H OCEAN|240084.28546409807|\n         INLAND|124805.39200122119|\n+---------------+------------------+\n\nimport org.apache.spark.sql.types._\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Rewrite the above question in SQL."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14d91417-961c-431d-a79d-d379fb2b84d2"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nhousing.createOrReplaceTempView(\"df\")\nspark.sql(\"SELECT ocean_proximity, avg(median_house_value) as avg_value FROM df GROUP BY ocean_proximity\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fea1125-6cb5-40eb-944d-6c9cdfbe2530"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------------+------------------+\n|ocean_proximity|         avg_value|\n+---------------+------------------+\n|         ISLAND|          380440.0|\n|     NEAR OCEAN|249433.97742663656|\n|       NEAR BAY|259212.31179039303|\n|      &lt;1H OCEAN|240084.28546409807|\n|         INLAND|124805.39200122119|\n+---------------+------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+------------------+\nocean_proximity|         avg_value|\n+---------------+------------------+\n         ISLAND|          380440.0|\n     NEAR OCEAN|249433.97742663656|\n       NEAR BAY|259212.31179039303|\n      &lt;1H OCEAN|240084.28546409807|\n         INLAND|124805.39200122119|\n+---------------+------------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2.5. Correlation among attributes\nPrint the correlation among the attributes `housing_median_age`, `total_rooms`, `median_house_value`, and `population`. To do so, first you need to put these attributes into one vector. Then, compute the standard correlation coefficient (Pearson) between every pair of attributes in this new vector. To make a vector of these attributes, you can use the `VectorAssembler` Transformer."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdd2d82f-2c40-42e1-bbcd-53f2e1ad75fd"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.ml.feature.VectorAssembler\n\nval va = new VectorAssembler()\n  .setInputCols(Array(\"housing_median_age\", \"total_rooms\", \"median_house_value\", \"population\"))\n  .setOutputCol(\"correlation_features\")\n\nval housingAttrs = va.transform(housing)\nhousingAttrs.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b53489be-7d19-4081-a772-f9ac5e395af5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"housingAttrs","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"longitude","type":"double","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"housing_median_age","type":"double","nullable":true,"metadata":{}},{"name":"total_rooms","type":"double","nullable":true,"metadata":{}},{"name":"total_bedrooms","type":"double","nullable":true,"metadata":{}},{"name":"population","type":"double","nullable":true,"metadata":{}},{"name":"households","type":"double","nullable":true,"metadata":{}},{"name":"median_income","type":"double","nullable":true,"metadata":{}},{"name":"median_house_value","type":"double","nullable":true,"metadata":{}},{"name":"ocean_proximity","type":"string","nullable":true,"metadata":{}},{"name":"correlation_features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"housing_median_age"},{"idx":1,"name":"total_rooms"},{"idx":2,"name":"median_house_value"},{"idx":3,"name":"population"}]},"num_attrs":4}}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|correlation_features|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|[41.0,880.0,45260...|\n|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|[21.0,7099.0,3585...|\n|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|[52.0,1467.0,3521...|\n|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|[52.0,1274.0,3413...|\n|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|[52.0,1627.0,3422...|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\nonly showing top 5 rows\n\nimport org.apache.spark.ml.feature.VectorAssembler\nva: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_aea599ae8488, handleInvalid=error, numInputCols=4\nhousingAttrs: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\nlongitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|correlation_features|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|[41.0,880.0,45260...|\n  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|[21.0,7099.0,3585...|\n  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|[52.0,1467.0,3521...|\n  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|[52.0,1274.0,3413...|\n  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|[52.0,1627.0,3422...|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\nonly showing top 5 rows\n\nimport org.apache.spark.ml.feature.VectorAssembler\nva: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_aea599ae8488, handleInvalid=error, numInputCols=4\nhousingAttrs: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.ml.linalg.Matrix\nimport org.apache.spark.ml.stat.Correlation\nimport org.apache.spark.sql.Row\n\nval Row(coeff: Matrix) = Correlation.corr(housingAttrs, \"correlation_features\").head\n\nprintln(s\"The standard correlation coefficient:\\n ${coeff}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"319964b6-317a-4268-9891-4c7bb0c41a72"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">The standard correlation coefficient:\n 1.0                   -0.3612622012223152  0.10562341249321029    -0.29624423977353675   \n-0.3612622012223152   1.0                  0.13415311380656375    0.8571259728659744     \n0.10562341249321029   0.13415311380656375  1.0                    -0.024649678888894997  \n-0.29624423977353675  0.8571259728659744   -0.024649678888894997  1.0                    \nimport org.apache.spark.ml.linalg.Matrix\nimport org.apache.spark.ml.stat.Correlation\nimport org.apache.spark.sql.Row\ncoeff: org.apache.spark.ml.linalg.Matrix =\n1.0                   -0.3612622012223152  0.10562341249321029    -0.29624423977353675\n-0.3612622012223152   1.0                  0.13415311380656375    0.8571259728659744\n0.10562341249321029   0.13415311380656375  1.0                    -0.024649678888894997\n-0.29624423977353675  0.8571259728659744   -0.024649678888894997  1.0\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The standard correlation coefficient:\n 1.0                   -0.3612622012223152  0.10562341249321029    -0.29624423977353675   \n-0.3612622012223152   1.0                  0.13415311380656375    0.8571259728659744     \n0.10562341249321029   0.13415311380656375  1.0                    -0.024649678888894997  \n-0.29624423977353675  0.8571259728659744   -0.024649678888894997  1.0                    \nimport org.apache.spark.ml.linalg.Matrix\nimport org.apache.spark.ml.stat.Correlation\nimport org.apache.spark.sql.Row\ncoeff: org.apache.spark.ml.linalg.Matrix =\n1.0                   -0.3612622012223152  0.10562341249321029    -0.29624423977353675\n-0.3612622012223152   1.0                  0.13415311380656375    0.8571259728659744\n0.10562341249321029   0.13415311380656375  1.0                    -0.024649678888894997\n-0.29624423977353675  0.8571259728659744   -0.024649678888894997  1.0\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2.6. Combine and make new attributes\nNow, let's try out various attribute combinations. In the given dataset, the total number of rooms in a block is not very useful, if we don't know how many households there are. What we really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful, and we want to compare it to the number of rooms. And the population per household seems like also an interesting attribute combination to look at. To do so, add the three new columns to the dataset as below. We will call the new dataset the `housingExtra`.\n```\nrooms_per_household = total_rooms / households\nbedrooms_per_room = total_bedrooms / total_rooms\npopulation_per_household = population / households\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6c7621f-577b-405f-b8e7-60e4543eb259"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nval housingCol1 = housing.withColumn(\"rooms_per_household\", $\"total_rooms\"/$\"households\")\nval housingCol2 = housingCol1.withColumn(\"bedrooms_per_room\", $\"total_bedrooms\"/$\"total_rooms\")\nval housingExtra = housingCol2.withColumn(\"population_per_household\", $\"population\"/$\"households\")\n\nhousingExtra.select(\"rooms_per_household\", \"bedrooms_per_room\", \"population_per_household\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"318a2e70-e441-457a-a260-bda67249e0cc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"housingCol1","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"longitude","type":"double","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"housing_median_age","type":"double","nullable":true,"metadata":{}},{"name":"total_rooms","type":"double","nullable":true,"metadata":{}},{"name":"total_bedrooms","type":"double","nullable":true,"metadata":{}},{"name":"population","type":"double","nullable":true,"metadata":{}},{"name":"households","type":"double","nullable":true,"metadata":{}},{"name":"median_income","type":"double","nullable":true,"metadata":{}},{"name":"median_house_value","type":"double","nullable":true,"metadata":{}},{"name":"ocean_proximity","type":"string","nullable":true,"metadata":{}},{"name":"rooms_per_household","type":"double","nullable":true,"metadata":{}}]},"tableIdentifier":null},{"name":"housingCol2","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"longitude","type":"double","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"housing_median_age","type":"double","nullable":true,"metadata":{}},{"name":"total_rooms","type":"double","nullable":true,"metadata":{}},{"name":"total_bedrooms","type":"double","nullable":true,"metadata":{}},{"name":"population","type":"double","nullable":true,"metadata":{}},{"name":"households","type":"double","nullable":true,"metadata":{}},{"name":"median_income","type":"double","nullable":true,"metadata":{}},{"name":"median_house_value","type":"double","nullable":true,"metadata":{}},{"name":"ocean_proximity","type":"string","nullable":true,"metadata":{}},{"name":"rooms_per_household","type":"double","nullable":true,"metadata":{}},{"name":"bedrooms_per_room","type":"double","nullable":true,"metadata":{}}]},"tableIdentifier":null},{"name":"housingExtra","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"longitude","type":"double","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"housing_median_age","type":"double","nullable":true,"metadata":{}},{"name":"total_rooms","type":"double","nullable":true,"metadata":{}},{"name":"total_bedrooms","type":"double","nullable":true,"metadata":{}},{"name":"population","type":"double","nullable":true,"metadata":{}},{"name":"households","type":"double","nullable":true,"metadata":{}},{"name":"median_income","type":"double","nullable":true,"metadata":{}},{"name":"median_house_value","type":"double","nullable":true,"metadata":{}},{"name":"ocean_proximity","type":"string","nullable":true,"metadata":{}},{"name":"rooms_per_household","type":"double","nullable":true,"metadata":{}},{"name":"bedrooms_per_room","type":"double","nullable":true,"metadata":{}},{"name":"population_per_household","type":"double","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-------------------+-------------------+------------------------+\n|rooms_per_household|  bedrooms_per_room|population_per_household|\n+-------------------+-------------------+------------------------+\n|  6.984126984126984|0.14659090909090908|      2.5555555555555554|\n|  6.238137082601054|0.15579659106916466|       2.109841827768014|\n|  8.288135593220339|0.12951601908657123|      2.8022598870056497|\n| 5.8173515981735155|0.18445839874411302|       2.547945205479452|\n|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|\n+-------------------+-------------------+------------------------+\nonly showing top 5 rows\n\nhousingCol1: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\nhousingCol2: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 10 more fields]\nhousingExtra: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-------------------+------------------------+\nrooms_per_household|  bedrooms_per_room|population_per_household|\n+-------------------+-------------------+------------------------+\n  6.984126984126984|0.14659090909090908|      2.5555555555555554|\n  6.238137082601054|0.15579659106916466|       2.109841827768014|\n  8.288135593220339|0.12951601908657123|      2.8022598870056497|\n 5.8173515981735155|0.18445839874411302|       2.547945205479452|\n  6.281853281853282| 0.1720958819913952|      2.1814671814671813|\n+-------------------+-------------------+------------------------+\nonly showing top 5 rows\n\nhousingCol1: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\nhousingCol2: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 10 more fields]\nhousingExtra: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["---\n## 3. Prepare the data for Machine Learning algorithms\nBefore going through the Machine Learning steps, let's first rename the label column from `median_house_value` to `label`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0690e917-ac13-47a0-9d92-1f1707f8eda0"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nval renamedHousing = housingExtra.withColumnRenamed(\"median_house_value\", \"label\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"499ce946-d2c0-40eb-a35c-c019322255fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"renamedHousing","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"longitude","type":"double","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"housing_median_age","type":"double","nullable":true,"metadata":{}},{"name":"total_rooms","type":"double","nullable":true,"metadata":{}},{"name":"total_bedrooms","type":"double","nullable":true,"metadata":{}},{"name":"population","type":"double","nullable":true,"metadata":{}},{"name":"households","type":"double","nullable":true,"metadata":{}},{"name":"median_income","type":"double","nullable":true,"metadata":{}},{"name":"label","type":"double","nullable":true,"metadata":{}},{"name":"ocean_proximity","type":"string","nullable":true,"metadata":{}},{"name":"rooms_per_household","type":"double","nullable":true,"metadata":{}},{"name":"bedrooms_per_room","type":"double","nullable":true,"metadata":{}},{"name":"population_per_household","type":"double","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">renamedHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">renamedHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now, we want to separate the numerical attributes from the categorical attribute (`ocean_proximity`) and keep their column names in two different lists. Moreover, sice we don't want to apply the same transformations to the predictors (features) and the label, we should remove the label attribute from the list of predictors."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fc1b2d7-9934-44e8-b97e-5853778d9235"}}},{"cell_type":"code","source":["%scala\n// label columns\nval colLabel = \"label\"\n\n// categorical columns\nval colCat = \"ocean_proximity\"\n\n// numerical columns\nval colNum = renamedHousing.columns.filter(_ != colLabel).filter(_ != colCat)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a538cddc-3662-46b5-94ea-d39fe535d243"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">colLabel: String = label\ncolCat: String = ocean_proximity\ncolNum: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, rooms_per_household, bedrooms_per_room, population_per_household)\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">colLabel: String = label\ncolCat: String = ocean_proximity\ncolNum: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, rooms_per_household, bedrooms_per_room, population_per_household)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 3.1. Prepare continuse attributes\n### Data cleaning\nMost Machine Learning algorithms cannot work with missing features, so we should take care of them. As a first step, let's find the columns with missing values in the numerical attributes. To do so, we can print the number of missing values of each continues attributes, listed in `colNum`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b975a829-7e6b-4e2b-b12d-06f24d54b53e"}}},{"cell_type":"code","source":["%scala\ncolNum"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78098441-b7d0-4abf-a127-c5295218895f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res17: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, rooms_per_household, bedrooms_per_room, population_per_household)\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res17: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, rooms_per_household, bedrooms_per_room, population_per_household)\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions._\n// TODO: Replace <FILL IN> with appropriate code\nvar tmp = renamedHousing.select(renamedHousing.columns.map(c => sum(col(c).isNull.cast(\"int\")).alias(c)): _*).select(colNum.map(col): _*).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7635074-1770-4e7e-b010-c39ddce28a6d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|rooms_per_household|bedrooms_per_room|population_per_household|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n|        0|       0|                 0|          0|           207|         0|         0|            0|                  0|              207|                       0|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n\nimport org.apache.spark.sql.functions._\ntmp: Unit = ()\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\nlongitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|rooms_per_household|bedrooms_per_room|population_per_household|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n        0|       0|                 0|          0|           207|         0|         0|            0|                  0|              207|                       0|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n\nimport org.apache.spark.sql.functions._\ntmp: Unit = ()\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["As we observerd above, the `total_bedrooms` and `bedrooms_per_room` attributes have some missing values. One way to take care of missing values is to use the `Imputer` Transformer, which completes missing values in a dataset, either using the mean or the median of the columns in which the missing values are located. To use it, you need to create an `Imputer` instance, specifying that you want to replace each attribute's missing values with the \"median\" of that attribute."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6277d360-a7d7-4613-b075-92202ba6864e"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.ml.feature.Imputer\n\nval imputer = new Imputer().setStrategy(\"median\")\n  .setInputCols(Array(\"total_bedrooms\", \"bedrooms_per_room\"))\n  .setOutputCols(Array(\"total_bedrooms\", \"bedrooms_per_room\"))\n\nval imputedHousing = imputer.fit(renamedHousing).transform(renamedHousing)\n  .select(\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"rooms_per_household\", \"bedrooms_per_room\", \"population_per_household\")\n\nimputedHousing.select(\"total_bedrooms\", \"bedrooms_per_room\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cee4677-9c55-4cf0-a48c-79d3dc54d125"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"imputedHousing","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"longitude","type":"double","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"housing_median_age","type":"double","nullable":true,"metadata":{}},{"name":"total_rooms","type":"double","nullable":true,"metadata":{}},{"name":"total_bedrooms","type":"double","nullable":true,"metadata":{}},{"name":"population","type":"double","nullable":true,"metadata":{}},{"name":"households","type":"double","nullable":true,"metadata":{}},{"name":"median_income","type":"double","nullable":true,"metadata":{}},{"name":"rooms_per_household","type":"double","nullable":true,"metadata":{}},{"name":"bedrooms_per_room","type":"double","nullable":true,"metadata":{}},{"name":"population_per_household","type":"double","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+--------------+-------------------+\n|total_bedrooms|  bedrooms_per_room|\n+--------------+-------------------+\n|         129.0|0.14659090909090908|\n|        1106.0|0.15579659106916466|\n|         190.0|0.12951601908657123|\n|         235.0|0.18445839874411302|\n|         280.0| 0.1720958819913952|\n+--------------+-------------------+\nonly showing top 5 rows\n\nimport org.apache.spark.ml.feature.Imputer\nimputer: org.apache.spark.ml.feature.Imputer = imputer_71541df7a727\nimputedHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------+-------------------+\ntotal_bedrooms|  bedrooms_per_room|\n+--------------+-------------------+\n         129.0|0.14659090909090908|\n        1106.0|0.15579659106916466|\n         190.0|0.12951601908657123|\n         235.0|0.18445839874411302|\n         280.0| 0.1720958819913952|\n+--------------+-------------------+\nonly showing top 5 rows\n\nimport org.apache.spark.ml.feature.Imputer\nimputer: org.apache.spark.ml.feature.Imputer = imputer_71541df7a727\nimputedHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Scaling\nOne of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don't perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the label attribues is generally not required.\n\nOne way to get all attributes to have the same scale is to use standardization. In standardization, for each value, first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the variance so that the resulting distribution has unit variance. To do this, we can use the `StandardScaler` Estimator. To use `StandardScaler`, again we need to convert all the numerical attributes into a big vectore of features using `VectorAssembler`, and then call `StandardScaler` on that vactor."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8175ef6c-fd71-47bf-9992-380e74278b05"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\nvar cols = Array(\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"rooms_per_household\", \"bedrooms_per_room\", \"population_per_household\")\nval va = new VectorAssembler()\n  .setInputCols(cols)\n  .setOutputCol(\"features\")\nval featuredHousing = va.transform(imputedHousing)\n\nval scaler = new StandardScaler()\n  .setInputCol(\"features\")\n  .setOutputCol(\"scaledFeatures\")\n  .setWithStd(true)\n  .setWithMean(false)\nval scaledHousing = scaler.fit(featuredHousing).transform(featuredHousing)\n\nscaledHousing.select(\"scaledFeatures\").show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c125a75e-e2cb-4f59-af48-b7bc3dec8108"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"featuredHousing","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"longitude","type":"double","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"housing_median_age","type":"double","nullable":true,"metadata":{}},{"name":"total_rooms","type":"double","nullable":true,"metadata":{}},{"name":"total_bedrooms","type":"double","nullable":true,"metadata":{}},{"name":"population","type":"double","nullable":true,"metadata":{}},{"name":"households","type":"double","nullable":true,"metadata":{}},{"name":"median_income","type":"double","nullable":true,"metadata":{}},{"name":"rooms_per_household","type":"double","nullable":true,"metadata":{}},{"name":"bedrooms_per_room","type":"double","nullable":true,"metadata":{}},{"name":"population_per_household","type":"double","nullable":true,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"longitude"},{"idx":1,"name":"latitude"},{"idx":2,"name":"housing_median_age"},{"idx":3,"name":"total_rooms"},{"idx":4,"name":"total_bedrooms"},{"idx":5,"name":"population"},{"idx":6,"name":"households"},{"idx":7,"name":"median_income"},{"idx":8,"name":"rooms_per_household"},{"idx":9,"name":"bedrooms_per_room"},{"idx":10,"name":"population_per_household"}]},"num_attrs":11}}}]},"tableIdentifier":null},{"name":"scaledHousing","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"longitude","type":"double","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"housing_median_age","type":"double","nullable":true,"metadata":{}},{"name":"total_rooms","type":"double","nullable":true,"metadata":{}},{"name":"total_bedrooms","type":"double","nullable":true,"metadata":{}},{"name":"population","type":"double","nullable":true,"metadata":{}},{"name":"households","type":"double","nullable":true,"metadata":{}},{"name":"median_income","type":"double","nullable":true,"metadata":{}},{"name":"rooms_per_household","type":"double","nullable":true,"metadata":{}},{"name":"bedrooms_per_room","type":"double","nullable":true,"metadata":{}},{"name":"population_per_household","type":"double","nullable":true,"metadata":{}},{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"longitude"},{"idx":1,"name":"latitude"},{"idx":2,"name":"housing_median_age"},{"idx":3,"name":"total_rooms"},{"idx":4,"name":"total_bedrooms"},{"idx":5,"name":"population"},{"idx":6,"name":"households"},{"idx":7,"name":"median_income"},{"idx":8,"name":"rooms_per_household"},{"idx":9,"name":"bedrooms_per_room"},{"idx":10,"name":"population_per_household"}]},"num_attrs":11}}},{"name":"scaledFeatures","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"num_attrs":11}}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+--------------------+\n|      scaledFeatures|\n+--------------------+\n|[-61.007269596069...|\n|[-61.002278409814...|\n|[-61.012260782324...|\n|[-61.017251968579...|\n|[-61.017251968579...|\n+--------------------+\nonly showing top 5 rows\n\nimport org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\ncols: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, rooms_per_household, bedrooms_per_room, population_per_household)\nva: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_4fd49ed3c02e, handleInvalid=error, numInputCols=11\nfeaturedHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 10 more fields]\nscaler: org.apache.spark.ml.feature.StandardScaler = stdScal_cc83759561f6\nscaledHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\n      scaledFeatures|\n+--------------------+\n[-61.007269596069...|\n[-61.002278409814...|\n[-61.012260782324...|\n[-61.017251968579...|\n[-61.017251968579...|\n+--------------------+\nonly showing top 5 rows\n\nimport org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\ncols: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, rooms_per_household, bedrooms_per_room, population_per_household)\nva: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_4fd49ed3c02e, handleInvalid=error, numInputCols=11\nfeaturedHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 10 more fields]\nscaler: org.apache.spark.ml.feature.StandardScaler = stdScal_cc83759561f6\nscaledHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n\nval va = new VectorAssembler().\nval featuredHousing = va.transform(imputedHousing)\n\nval scaler = new StandardScaler().<FILL IN>\nval scaledHousing = scaler.fit(featuredHousing).transform(featuredHousing)\n\nscaledHousing.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cba6cb8-f925-4e36-a72d-d7ca56eeea1c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"<div class=\"ansiout\">&lt;console&gt;:10: error: identifier expected but 'val' found.\n       val featuredHousing = va.transform(imputedHousing)\n       ^\n</div>","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 3.2. Prepare categorical attributes\nAfter imputing and scaling the continuse attributes, we should take care of the categorical attributes. Let's first print the number of distict values of the categirical attribute `ocean_proximity`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"606190b8-1c95-4b65-b85b-465a611a30e0"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nrenamedHousing.select(\"ocean_proximity\").distinct.show"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eadda118-46d2-4347-a11f-c2da1ceea578"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------------+\n|ocean_proximity|\n+---------------+\n|         ISLAND|\n|     NEAR OCEAN|\n|       NEAR BAY|\n|      &lt;1H OCEAN|\n|         INLAND|\n+---------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+\nocean_proximity|\n+---------------+\n         ISLAND|\n     NEAR OCEAN|\n       NEAR BAY|\n      &lt;1H OCEAN|\n         INLAND|\n+---------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### String indexer\nMost Machine Learning algorithms prefer to work with numbers. So let's convert the categorical attribute `ocean_proximity` to numbers. To do so, we can use the `StringIndexer` that encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2dd3a02-1ae6-4662-8299-21bc9c1fb2a0"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.ml.feature.StringIndexer\n\nval indexer = new StringIndexer().setInputCol(\"ocean_proximity\").setOutputCol(\"ocean_proximity_indexed\")\nval idxHousing = indexer.fit(renamedHousing).transform(renamedHousing)\n\nidxHousing.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f3bca6d-db24-41a5-b30a-d39611fec3a4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"idxHousing","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"longitude","type":"double","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"housing_median_age","type":"double","nullable":true,"metadata":{}},{"name":"total_rooms","type":"double","nullable":true,"metadata":{}},{"name":"total_bedrooms","type":"double","nullable":true,"metadata":{}},{"name":"population","type":"double","nullable":true,"metadata":{}},{"name":"households","type":"double","nullable":true,"metadata":{}},{"name":"median_income","type":"double","nullable":true,"metadata":{}},{"name":"label","type":"double","nullable":true,"metadata":{}},{"name":"ocean_proximity","type":"string","nullable":true,"metadata":{}},{"name":"rooms_per_household","type":"double","nullable":true,"metadata":{}},{"name":"bedrooms_per_room","type":"double","nullable":true,"metadata":{}},{"name":"population_per_household","type":"double","nullable":true,"metadata":{}},{"name":"ocean_proximity_indexed","type":"double","nullable":false,"metadata":{"ml_attr":{"vals":["<1H OCEAN","INLAND","NEAR OCEAN","NEAR BAY","ISLAND"],"type":"nominal","name":"ocean_proximity_indexed"}}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|ocean_proximity_indexed|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\n|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|                    3.0|\n|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|                    3.0|\n|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|                    3.0|\n|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|                    3.0|\n|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|                    3.0|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\nonly showing top 5 rows\n\nimport org.apache.spark.ml.feature.StringIndexer\nindexer: org.apache.spark.ml.feature.StringIndexer = strIdx_69469f6c2269\nidxHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 12 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\nlongitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|ocean_proximity_indexed|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\n  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|                    3.0|\n  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|                    3.0|\n  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|                    3.0|\n  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|                    3.0|\n  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|                    3.0|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+\nonly showing top 5 rows\n\nimport org.apache.spark.ml.feature.StringIndexer\nindexer: org.apache.spark.ml.feature.StringIndexer = strIdx_69469f6c2269\nidxHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 12 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now we can use this numerical data in any Machine Learning algorithm. You can look at the mapping that this encoder has learned using the `labels` method: \"<1H OCEAN\" is mapped to 0, \"INLAND\" is mapped to 1, etc."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c921357b-4e85-4669-a632-b5c46b4d690f"}}},{"cell_type":"code","source":["%scala\nindexer.fit(renamedHousing).labelsArray"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5143a490-de8f-4296-9845-7cc0529f1fda"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res28: Array[Array[String]] = Array(Array(&lt;1H OCEAN, INLAND, NEAR OCEAN, NEAR BAY, ISLAND))\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res28: Array[Array[String]] = Array(Array(&lt;1H OCEAN, INLAND, NEAR OCEAN, NEAR BAY, ISLAND))\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### One-hot encoding\nNow, convert the label indices built in the last step into one-hot vectors. To do this, you can take advantage of the `OneHotEncoderEstimator` Estimator."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83135682-0d40-417d-9aed-7d5a4a273fee"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.ml.feature.OneHotEncoder\n\nval encoder = new OneHotEncoder()\n  .setInputCol(\"ocean_proximity_indexed\")\n  .setOutputCol(\"ocean_prox_encoded\")\nval ohHousing = encoder.fit(idxHousing).transform(idxHousing)\n\nohHousing.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b14d6445-f876-44f7-8f07-79eead8417af"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"ohHousing","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"longitude","type":"double","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"housing_median_age","type":"double","nullable":true,"metadata":{}},{"name":"total_rooms","type":"double","nullable":true,"metadata":{}},{"name":"total_bedrooms","type":"double","nullable":true,"metadata":{}},{"name":"population","type":"double","nullable":true,"metadata":{}},{"name":"households","type":"double","nullable":true,"metadata":{}},{"name":"median_income","type":"double","nullable":true,"metadata":{}},{"name":"label","type":"double","nullable":true,"metadata":{}},{"name":"ocean_proximity","type":"string","nullable":true,"metadata":{}},{"name":"rooms_per_household","type":"double","nullable":true,"metadata":{}},{"name":"bedrooms_per_room","type":"double","nullable":true,"metadata":{}},{"name":"population_per_household","type":"double","nullable":true,"metadata":{}},{"name":"ocean_proximity_indexed","type":"double","nullable":false,"metadata":{"ml_attr":{"vals":["<1H OCEAN","INLAND","NEAR OCEAN","NEAR BAY","ISLAND"],"type":"nominal","name":"ocean_proximity_indexed"}}},{"name":"ocean_prox_encoded","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"attrs":{"binary":[{"idx":0,"name":"<1H OCEAN"},{"idx":1,"name":"INLAND"},{"idx":2,"name":"NEAR OCEAN"},{"idx":3,"name":"NEAR BAY"}]},"num_attrs":4}}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+------------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|ocean_proximity_indexed|ocean_prox_encoded|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+------------------+\n|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|                    3.0|     (4,[3],[1.0])|\n|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|                    3.0|     (4,[3],[1.0])|\n|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|                    3.0|     (4,[3],[1.0])|\n|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|                    3.0|     (4,[3],[1.0])|\n|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|                    3.0|     (4,[3],[1.0])|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+------------------+\nonly showing top 5 rows\n\nimport org.apache.spark.ml.feature.OneHotEncoder\nencoder: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_c4365c92df56\nohHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 13 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+------------------+\nlongitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|ocean_proximity_indexed|ocean_prox_encoded|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+------------------+\n  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|                    3.0|     (4,[3],[1.0])|\n  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|                    3.0|     (4,[3],[1.0])|\n  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|                    3.0|     (4,[3],[1.0])|\n  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|                    3.0|     (4,[3],[1.0])|\n  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|                    3.0|     (4,[3],[1.0])|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+-----------------------+------------------+\nonly showing top 5 rows\n\nimport org.apache.spark.ml.feature.OneHotEncoder\nencoder: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_c4365c92df56\nohHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 13 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["---\n# 4. Pipeline\nAs you can see, there are many data transformation steps that need to be executed in the right order. For example, you called the `Imputer`, `VectorAssembler`, and `StandardScaler` from left to right. However, we can use the `Pipeline` class to define a sequence of Transformers/Estimators, and run them in order. A `Pipeline` is an `Estimator`, thus, after a Pipeline's `fit()` method runs, it produces a `PipelineModel`, which is a `Transformer`.\n\nNow, let's create a pipeline called `numPipeline` to call the numerical transformers you built above (`imputer`, `va`, and `scaler`) in the right order from left to right, as well as a pipeline called `catPipeline` to call the categorical transformers (`indexer` and `encoder`). Then, put these two pipelines `numPipeline` and `catPipeline` into one pipeline."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"281143ec-74e3-4f99-9e76-e906aee8b56c"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\n\nval numPipeline = new Pipeline().setStages(Array(imputer, va, scaler))\nval catPipeline = new Pipeline().setStages(Array(indexer, encoder))\nval pipeline = new Pipeline().setStages(Array(numPipeline, catPipeline))\nval newHousing = pipeline.fit(renamedHousing).transform(renamedHousing).drop(\"features\")\n\nnewHousing.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f53e06d-92ba-4925-84d4-860dba34acaa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"newHousing","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"longitude","type":"double","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"housing_median_age","type":"double","nullable":true,"metadata":{}},{"name":"total_rooms","type":"double","nullable":true,"metadata":{}},{"name":"total_bedrooms","type":"double","nullable":true,"metadata":{}},{"name":"population","type":"double","nullable":true,"metadata":{}},{"name":"households","type":"double","nullable":true,"metadata":{}},{"name":"median_income","type":"double","nullable":true,"metadata":{}},{"name":"label","type":"double","nullable":true,"metadata":{}},{"name":"ocean_proximity","type":"string","nullable":true,"metadata":{}},{"name":"rooms_per_household","type":"double","nullable":true,"metadata":{}},{"name":"bedrooms_per_room","type":"double","nullable":true,"metadata":{}},{"name":"population_per_household","type":"double","nullable":true,"metadata":{}},{"name":"scaledFeatures","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"num_attrs":11}}},{"name":"ocean_proximity_indexed","type":"double","nullable":false,"metadata":{"ml_attr":{"vals":["<1H OCEAN","INLAND","NEAR OCEAN","NEAR BAY","ISLAND"],"type":"nominal","name":"ocean_proximity_indexed"}}},{"name":"ocean_prox_encoded","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"attrs":{"binary":[{"idx":0,"name":"<1H OCEAN"},{"idx":1,"name":"INLAND"},{"idx":2,"name":"NEAR OCEAN"},{"idx":3,"name":"NEAR BAY"}]},"num_attrs":4}}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+-----------------------+------------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|      scaledFeatures|ocean_proximity_indexed|ocean_prox_encoded|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+-----------------------+------------------+\n|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|[-61.007269596069...|                    3.0|     (4,[3],[1.0])|\n|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|[-61.002278409814...|                    3.0|     (4,[3],[1.0])|\n|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|[-61.012260782324...|                    3.0|     (4,[3],[1.0])|\n|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|[-61.017251968579...|                    3.0|     (4,[3],[1.0])|\n|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|[-61.017251968579...|                    3.0|     (4,[3],[1.0])|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+-----------------------+------------------+\nonly showing top 5 rows\n\nimport org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\nnumPipeline: org.apache.spark.ml.Pipeline = pipeline_22caaa728db1\ncatPipeline: org.apache.spark.ml.Pipeline = pipeline_586f17ce55bf\npipeline: org.apache.spark.ml.Pipeline = pipeline_90267414e756\nnewHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 14 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+-----------------------+------------------+\nlongitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|      scaledFeatures|ocean_proximity_indexed|ocean_prox_encoded|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+-----------------------+------------------+\n  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|[-61.007269596069...|                    3.0|     (4,[3],[1.0])|\n  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|358500.0|       NEAR BAY|  6.238137082601054|0.15579659106916466|       2.109841827768014|[-61.002278409814...|                    3.0|     (4,[3],[1.0])|\n  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|352100.0|       NEAR BAY|  8.288135593220339|0.12951601908657123|      2.8022598870056497|[-61.012260782324...|                    3.0|     (4,[3],[1.0])|\n  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|341300.0|       NEAR BAY| 5.8173515981735155|0.18445839874411302|       2.547945205479452|[-61.017251968579...|                    3.0|     (4,[3],[1.0])|\n  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|342200.0|       NEAR BAY|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|[-61.017251968579...|                    3.0|     (4,[3],[1.0])|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+-----------------------+------------------+\nonly showing top 5 rows\n\nimport org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\nnumPipeline: org.apache.spark.ml.Pipeline = pipeline_22caaa728db1\ncatPipeline: org.apache.spark.ml.Pipeline = pipeline_586f17ce55bf\npipeline: org.apache.spark.ml.Pipeline = pipeline_90267414e756\nnewHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 14 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now, use `VectorAssembler` to put all attributes of the final dataset `newHousing` into a big vector, and call the new column `features`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9aaa654b-7ae6-471a-b13c-fea0f3322f6c"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nval va2 = new VectorAssembler()\n  .setInputCols(Array(\"scaledFeatures\", \"ocean_prox_encoded\"))\n  .setOutputCol(\"features\")\nval dataset = va2.transform(newHousing).select(\"features\", \"label\")\n\ndataset.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20d9f4b2-9879-4270-9c8b-97976a764d17"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"dataset","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"scaledFeatures_0"},{"idx":1,"name":"scaledFeatures_1"},{"idx":2,"name":"scaledFeatures_2"},{"idx":3,"name":"scaledFeatures_3"},{"idx":4,"name":"scaledFeatures_4"},{"idx":5,"name":"scaledFeatures_5"},{"idx":6,"name":"scaledFeatures_6"},{"idx":7,"name":"scaledFeatures_7"},{"idx":8,"name":"scaledFeatures_8"},{"idx":9,"name":"scaledFeatures_9"},{"idx":10,"name":"scaledFeatures_10"}],"binary":[{"idx":11,"name":"ocean_prox_encoded_<1H OCEAN"},{"idx":12,"name":"ocean_prox_encoded_INLAND"},{"idx":13,"name":"ocean_prox_encoded_NEAR OCEAN"},{"idx":14,"name":"ocean_prox_encoded_NEAR BAY"}]},"num_attrs":15}}},{"name":"label","type":"double","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+--------------------+--------+\n|            features|   label|\n+--------------------+--------+\n|[-61.007269596069...|452600.0|\n|[-61.002278409814...|358500.0|\n|[-61.012260782324...|352100.0|\n|[-61.017251968579...|341300.0|\n|[-61.017251968579...|342200.0|\n+--------------------+--------+\nonly showing top 5 rows\n\nva2: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_447621776157, handleInvalid=error, numInputCols=2\ndataset: org.apache.spark.sql.DataFrame = [features: vector, label: double]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------+\n            features|   label|\n+--------------------+--------+\n[-61.007269596069...|452600.0|\n[-61.002278409814...|358500.0|\n[-61.012260782324...|352100.0|\n[-61.017251968579...|341300.0|\n[-61.017251968579...|342200.0|\n+--------------------+--------+\nonly showing top 5 rows\n\nva2: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_447621776157, handleInvalid=error, numInputCols=2\ndataset: org.apache.spark.sql.DataFrame = [features: vector, label: double]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["---\n# 5. Make a model\nHere we going to make four different regression models:\n* Linear regression model\n* Decission tree regression\n* Random forest regression\n* Gradient-booster forest regression\n\nBut, before giving the data to train a Machine Learning model, let's first split the data into training dataset (`trainSet`) with 80% of the whole data, and test dataset (`testSet`) with 20% of it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4344ef67-740f-43c7-84a5-8ac613c614b3"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nval Array(trainSet, testSet) = dataset.randomSplit(Array(0.8, 0.2))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"391962d8-744c-42f1-89c5-1a0e47158c52"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"trainSet","typeStr":"org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]","schema":{"type":"struct","fields":[{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"scaledFeatures_0"},{"idx":1,"name":"scaledFeatures_1"},{"idx":2,"name":"scaledFeatures_2"},{"idx":3,"name":"scaledFeatures_3"},{"idx":4,"name":"scaledFeatures_4"},{"idx":5,"name":"scaledFeatures_5"},{"idx":6,"name":"scaledFeatures_6"},{"idx":7,"name":"scaledFeatures_7"},{"idx":8,"name":"scaledFeatures_8"},{"idx":9,"name":"scaledFeatures_9"},{"idx":10,"name":"scaledFeatures_10"}],"binary":[{"idx":11,"name":"ocean_prox_encoded_<1H OCEAN"},{"idx":12,"name":"ocean_prox_encoded_INLAND"},{"idx":13,"name":"ocean_prox_encoded_NEAR OCEAN"},{"idx":14,"name":"ocean_prox_encoded_NEAR BAY"}]},"num_attrs":15}}},{"name":"label","type":"double","nullable":true,"metadata":{}}]},"tableIdentifier":null},{"name":"testSet","typeStr":"org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]","schema":{"type":"struct","fields":[{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"scaledFeatures_0"},{"idx":1,"name":"scaledFeatures_1"},{"idx":2,"name":"scaledFeatures_2"},{"idx":3,"name":"scaledFeatures_3"},{"idx":4,"name":"scaledFeatures_4"},{"idx":5,"name":"scaledFeatures_5"},{"idx":6,"name":"scaledFeatures_6"},{"idx":7,"name":"scaledFeatures_7"},{"idx":8,"name":"scaledFeatures_8"},{"idx":9,"name":"scaledFeatures_9"},{"idx":10,"name":"scaledFeatures_10"}],"binary":[{"idx":11,"name":"ocean_prox_encoded_<1H OCEAN"},{"idx":12,"name":"ocean_prox_encoded_INLAND"},{"idx":13,"name":"ocean_prox_encoded_NEAR OCEAN"},{"idx":14,"name":"ocean_prox_encoded_NEAR BAY"}]},"num_attrs":15}}},{"name":"label","type":"double","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">trainSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: double]\ntestSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: double]\nres62: Long = 4111\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">trainSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: double]\ntestSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: double]\nres62: Long = 4111\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 5.1. Linear regression model\nNow, train a Linear Regression model using the `LinearRegression` class. Then, print the coefficients and intercept of the model, as well as the summary of the model over the training set by calling the `summary` method."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e78e53df-abc8-4175-8c2b-07f3fc214f5e"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.ml.regression.LinearRegression\n\n// train the model\nval lr = new LinearRegression().setMaxIter(10).setElasticNetParam(0.1).setRegParam(0.5)\nval lrModel = lr.fit(trainSet)\nval trainingSummary = lrModel.summary\n\nprintln(s\"Coefficients: ${lrModel.coefficients}, Intercept: ${lrModel.intercept}\")\nprintln(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\nprintln(s\"R-squared: ${trainingSummary.r2}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d196d875-acb4-43fd-bd17-25f6933fc49f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Coefficients: [-18072.82798239788,-15001.831141742388,14523.97307818654,6510.725804081123,18641.16881061436,-37444.461608991165,18162.191450056995,82693.66653897235,2993.474372421311,18881.001876267386,-90.81415068561454,16998.900699994992,-36841.6507135625,28362.389732228723,17386.297007337616], Intercept: -909423.0712209494\nRMSE: 68256.17694962754\nR-squared: 0.651661392460911\nimport org.apache.spark.ml.regression.LinearRegression\nlr: org.apache.spark.ml.regression.LinearRegression = linReg_2d8613e20454\nlrModel: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_2d8613e20454, numFeatures=15\ntrainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@6da9e7ca\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Coefficients: [-18072.82798239788,-15001.831141742388,14523.97307818654,6510.725804081123,18641.16881061436,-37444.461608991165,18162.191450056995,82693.66653897235,2993.474372421311,18881.001876267386,-90.81415068561454,16998.900699994992,-36841.6507135625,28362.389732228723,17386.297007337616], Intercept: -909423.0712209494\nRMSE: 68256.17694962754\nR-squared: 0.651661392460911\nimport org.apache.spark.ml.regression.LinearRegression\nlr: org.apache.spark.ml.regression.LinearRegression = linReg_2d8613e20454\nlrModel: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_2d8613e20454, numFeatures=15\ntrainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@6da9e7ca\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now, use `RegressionEvaluator` to measure the root-mean-square-erroe (RMSE) of the model on the test dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4711bd1-9f29-4759-9234-abaafe58c870"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\n\n// make predictions on the test data\nval predictions = lrModel.transform(testSet)\npredictions.select(\"prediction\", \"label\", \"features\").show(5)\n\n// select (prediction, true label) and compute test error.\nval evaluator = new RegressionEvaluator()\n  .setMetricName(\"rmse\")\n  .setPredictionCol(\"prediction\")\n  .setLabelCol(\"label\")\nval rmse = evaluator.evaluate(predictions)\nprintln(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd11ec04-956a-4934-bca2-ae93d889a685"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"predictions","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"scaledFeatures_0"},{"idx":1,"name":"scaledFeatures_1"},{"idx":2,"name":"scaledFeatures_2"},{"idx":3,"name":"scaledFeatures_3"},{"idx":4,"name":"scaledFeatures_4"},{"idx":5,"name":"scaledFeatures_5"},{"idx":6,"name":"scaledFeatures_6"},{"idx":7,"name":"scaledFeatures_7"},{"idx":8,"name":"scaledFeatures_8"},{"idx":9,"name":"scaledFeatures_9"},{"idx":10,"name":"scaledFeatures_10"}],"binary":[{"idx":11,"name":"ocean_prox_encoded_<1H OCEAN"},{"idx":12,"name":"ocean_prox_encoded_INLAND"},{"idx":13,"name":"ocean_prox_encoded_NEAR OCEAN"},{"idx":14,"name":"ocean_prox_encoded_NEAR BAY"}]},"num_attrs":15}}},{"name":"label","type":"double","nullable":true,"metadata":{}},{"name":"prediction","type":"double","nullable":false,"metadata":{"ml_attr":{}}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+------------------+--------+--------------------+\n|        prediction|   label|            features|\n+------------------+--------+--------------------+\n|214144.81923173636| 94600.0|[-62.065401082150...|\n|181992.28784159466|103600.0|[-62.040445150874...|\n|222163.02506525663|106700.0|[-62.005506847089...|\n| 217628.7971001739|128900.0|[-61.975559729558...|\n| 152837.1718887746| 68300.0|[-61.975559729558...|\n+------------------+--------+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 70736.70392724255\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\npredictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_529817216354, metricName=rmse, throughOrigin=false\nrmse: Double = 70736.70392724255\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+--------+--------------------+\n        prediction|   label|            features|\n+------------------+--------+--------------------+\n214144.81923173636| 94600.0|[-62.065401082150...|\n181992.28784159466|103600.0|[-62.040445150874...|\n222163.02506525663|106700.0|[-62.005506847089...|\n 217628.7971001739|128900.0|[-61.975559729558...|\n 152837.1718887746| 68300.0|[-61.975559729558...|\n+------------------+--------+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 70736.70392724255\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\npredictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_529817216354, metricName=rmse, throughOrigin=false\nrmse: Double = 70736.70392724255\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 5.2. Decision tree regression\nRepeat what you have done on Regression Model to build a Decision Tree model. Use the `DecisionTreeRegressor` to make a model and then measure its RMSE on the test dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1d48b44-1a90-4c2e-be9a-822adeacb5ab"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\n\nval dtr = new DecisionTreeRegressor()\n  .setLabelCol(\"label\")\n  .setFeaturesCol(\"features\")\n// train the model\nval dtModel = dtr.fit(trainSet)\n\n// make predictions on the test data\nval predictions = dtModel.transform(testSet)\npredictions.select(\"prediction\", \"label\", \"features\").show(5)\n\n// select (prediction, true label) and compute test error\nval evaluator = new RegressionEvaluator()\n  .setMetricName(\"rmse\")\n  .setPredictionCol(\"prediction\")\n  .setLabelCol(\"label\")\nval rmse = evaluator.evaluate(predictions)\nprintln(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acda2be3-7d8b-4718-a9a6-0a1c3e65b687"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"predictions","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"scaledFeatures_0"},{"idx":1,"name":"scaledFeatures_1"},{"idx":2,"name":"scaledFeatures_2"},{"idx":3,"name":"scaledFeatures_3"},{"idx":4,"name":"scaledFeatures_4"},{"idx":5,"name":"scaledFeatures_5"},{"idx":6,"name":"scaledFeatures_6"},{"idx":7,"name":"scaledFeatures_7"},{"idx":8,"name":"scaledFeatures_8"},{"idx":9,"name":"scaledFeatures_9"},{"idx":10,"name":"scaledFeatures_10"}],"binary":[{"idx":11,"name":"ocean_prox_encoded_<1H OCEAN"},{"idx":12,"name":"ocean_prox_encoded_INLAND"},{"idx":13,"name":"ocean_prox_encoded_NEAR OCEAN"},{"idx":14,"name":"ocean_prox_encoded_NEAR BAY"}]},"num_attrs":15}}},{"name":"label","type":"double","nullable":true,"metadata":{}},{"name":"prediction","type":"double","nullable":false,"metadata":{"ml_attr":{}}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+------------------+--------+--------------------+\n|        prediction|   label|            features|\n+------------------+--------+--------------------+\n|  166668.141723356| 94600.0|[-62.065401082150...|\n|  166668.141723356|103600.0|[-62.040445150874...|\n|  166668.141723356|106700.0|[-62.005506847089...|\n|225471.38360175694|128900.0|[-61.975559729558...|\n|131986.66666666666| 68300.0|[-61.975559729558...|\n+------------------+--------+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 68805.05776623574\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\ndtr: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_cd55ef6c4fb5\ndtModel: org.apache.spark.ml.regression.DecisionTreeRegressionModel = DecisionTreeRegressionModel: uid=dtr_cd55ef6c4fb5, depth=5, numNodes=63, numFeatures=15\npredictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_c1ab9f2dfcd1, metricName=rmse, throughOrigin=false\nrmse: Double = 68805.05776623574\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+--------+--------------------+\n        prediction|   label|            features|\n+------------------+--------+--------------------+\n  166668.141723356| 94600.0|[-62.065401082150...|\n  166668.141723356|103600.0|[-62.040445150874...|\n  166668.141723356|106700.0|[-62.005506847089...|\n225471.38360175694|128900.0|[-61.975559729558...|\n131986.66666666666| 68300.0|[-61.975559729558...|\n+------------------+--------+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 68805.05776623574\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\ndtr: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_cd55ef6c4fb5\ndtModel: org.apache.spark.ml.regression.DecisionTreeRegressionModel = DecisionTreeRegressionModel: uid=dtr_cd55ef6c4fb5, depth=5, numNodes=63, numFeatures=15\npredictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_c1ab9f2dfcd1, metricName=rmse, throughOrigin=false\nrmse: Double = 68805.05776623574\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 5.3. Random forest regression\nLet's try the test error on a Random Forest Model. Youcan use the `RandomForestRegressor` to make a Random Forest model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3337877c-4b59-481b-9054-444b8d0b490d"}}},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.ml.regression.RandomForestRegressor\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\n\nval rf = new RandomForestRegressor()\n  .setLabelCol(\"label\")\n  .setFeaturesCol(\"features\")\n// train the model\nval rfModel = rf.fit(trainSet)\n\n// make predictions on the test data\nval predictions = rfModel.transform(testSet)\npredictions.select(\"prediction\", \"label\", \"features\").show(5)\n\n// select (prediction, true label) and compute test error\nval evaluator = new RegressionEvaluator()\n  .setMetricName(\"rmse\")\n  .setPredictionCol(\"prediction\")\n  .setLabelCol(\"label\")\nval rmse = evaluator.evaluate(predictions)\nprintln(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"feb6ac24-2a58-4aad-8715-1e063493826e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"predictions","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"scaledFeatures_0"},{"idx":1,"name":"scaledFeatures_1"},{"idx":2,"name":"scaledFeatures_2"},{"idx":3,"name":"scaledFeatures_3"},{"idx":4,"name":"scaledFeatures_4"},{"idx":5,"name":"scaledFeatures_5"},{"idx":6,"name":"scaledFeatures_6"},{"idx":7,"name":"scaledFeatures_7"},{"idx":8,"name":"scaledFeatures_8"},{"idx":9,"name":"scaledFeatures_9"},{"idx":10,"name":"scaledFeatures_10"}],"binary":[{"idx":11,"name":"ocean_prox_encoded_<1H OCEAN"},{"idx":12,"name":"ocean_prox_encoded_INLAND"},{"idx":13,"name":"ocean_prox_encoded_NEAR OCEAN"},{"idx":14,"name":"ocean_prox_encoded_NEAR BAY"}]},"num_attrs":15}}},{"name":"label","type":"double","nullable":true,"metadata":{}},{"name":"prediction","type":"double","nullable":false,"metadata":{"ml_attr":{}}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+------------------+--------+--------------------+\n|        prediction|   label|            features|\n+------------------+--------+--------------------+\n|225291.25984759885| 94600.0|[-62.065401082150...|\n|170707.01610683525|103600.0|[-62.040445150874...|\n|217920.82672573806|106700.0|[-62.005506847089...|\n|239978.98921641853|128900.0|[-61.975559729558...|\n| 182416.6447512183| 68300.0|[-61.975559729558...|\n+------------------+--------+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 67226.3826319314\nimport org.apache.spark.ml.regression.RandomForestRegressor\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nrf: org.apache.spark.ml.regression.RandomForestRegressor = rfr_f74e4657d0a5\nrfModel: org.apache.spark.ml.regression.RandomForestRegressionModel = RandomForestRegressionModel: uid=rfr_f74e4657d0a5, numTrees=20, numFeatures=15\npredictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_97b5b99a9f8a, metricName=rmse, throughOrigin=false\nrmse: Double = 67226.3826319314\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+--------+--------------------+\n        prediction|   label|            features|\n+------------------+--------+--------------------+\n225291.25984759885| 94600.0|[-62.065401082150...|\n170707.01610683525|103600.0|[-62.040445150874...|\n217920.82672573806|106700.0|[-62.005506847089...|\n239978.98921641853|128900.0|[-61.975559729558...|\n 182416.6447512183| 68300.0|[-61.975559729558...|\n+------------------+--------+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 67226.3826319314\nimport org.apache.spark.ml.regression.RandomForestRegressor\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nrf: org.apache.spark.ml.regression.RandomForestRegressor = rfr_f74e4657d0a5\nrfModel: org.apache.spark.ml.regression.RandomForestRegressionModel = RandomForestRegressionModel: uid=rfr_f74e4657d0a5, numTrees=20, numFeatures=15\npredictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_97b5b99a9f8a, metricName=rmse, throughOrigin=false\nrmse: Double = 67226.3826319314\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 5.4. Gradient-boosted tree regression\nFianlly, we want to build a Gradient-boosted Tree Regression model and test the RMSE of the test data. Use the `GBTRegressor` to build the model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d89f3af-af54-4785-b71a-d483f35baab3"}}},{"cell_type":"code","source":["%scala\ntrainSet.show(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a48f1214-d00c-4aac-8109-2dcbb07347ae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------------+-------+\n|            features|  label|\n+--------------------+-------+\n|[-62.040445150874...|85800.0|\n+--------------------+-------+\nonly showing top 1 row\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+-------+\n            features|  label|\n+--------------------+-------+\n[-62.040445150874...|85800.0|\n+--------------------+-------+\nonly showing top 1 row\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.ml.regression.GBTRegressor\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\n\nval gb = new GBTRegressor()\n  .setLabelCol(\"label\")\n  .setFeaturesCol(\"features\")\n// train the model\nval gbModel = gb.fit(trainSet)\n\n// make predictions on the test data\nval predictions = gbModel.transform(testSet)\npredictions.select(\"prediction\", \"label\", \"features\").show(5)\n\n// select (prediction, true label) and compute test error\nval evaluator = new RegressionEvaluator()\n  .setMetricName(\"rmse\")\n  .setPredictionCol(\"prediction\")\n  .setLabelCol(\"label\")\nval rmse = evaluator.evaluate(predictions)\nprintln(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"100afcb4-1bb3-4693-bcfd-527f2888417a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"predictions","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"scaledFeatures_0"},{"idx":1,"name":"scaledFeatures_1"},{"idx":2,"name":"scaledFeatures_2"},{"idx":3,"name":"scaledFeatures_3"},{"idx":4,"name":"scaledFeatures_4"},{"idx":5,"name":"scaledFeatures_5"},{"idx":6,"name":"scaledFeatures_6"},{"idx":7,"name":"scaledFeatures_7"},{"idx":8,"name":"scaledFeatures_8"},{"idx":9,"name":"scaledFeatures_9"},{"idx":10,"name":"scaledFeatures_10"}],"binary":[{"idx":11,"name":"ocean_prox_encoded_<1H OCEAN"},{"idx":12,"name":"ocean_prox_encoded_INLAND"},{"idx":13,"name":"ocean_prox_encoded_NEAR OCEAN"},{"idx":14,"name":"ocean_prox_encoded_NEAR BAY"}]},"num_attrs":15}}},{"name":"label","type":"double","nullable":true,"metadata":{}},{"name":"prediction","type":"double","nullable":false,"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"scaledFeatures_0"},{"idx":1,"name":"scaledFeatures_1"},{"idx":2,"name":"scaledFeatures_2"},{"idx":3,"name":"scaledFeatures_3"},{"idx":4,"name":"scaledFeatures_4"},{"idx":5,"name":"scaledFeatures_5"},{"idx":6,"name":"scaledFeatures_6"},{"idx":7,"name":"scaledFeatures_7"},{"idx":8,"name":"scaledFeatures_8"},{"idx":9,"name":"scaledFeatures_9"},{"idx":10,"name":"scaledFeatures_10"}],"binary":[{"idx":11,"name":"ocean_prox_encoded_<1H OCEAN"},{"idx":12,"name":"ocean_prox_encoded_INLAND"},{"idx":13,"name":"ocean_prox_encoded_NEAR OCEAN"},{"idx":14,"name":"ocean_prox_encoded_NEAR BAY"}]},"num_attrs":15}}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+------------------+--------+--------------------+\n|        prediction|   label|            features|\n+------------------+--------+--------------------+\n| 154172.2986563275| 94600.0|[-62.065401082150...|\n|110296.36987270221|103600.0|[-62.040445150874...|\n|144209.55530902446|106700.0|[-62.005506847089...|\n|123196.33176988826|128900.0|[-61.975559729558...|\n| 74594.01349042215| 68300.0|[-61.975559729558...|\n+------------------+--------+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 57477.9022016309\nimport org.apache.spark.ml.regression.GBTRegressor\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\ngb: org.apache.spark.ml.regression.GBTRegressor = gbtr_d4d0e62c6394\ngbModel: org.apache.spark.ml.regression.GBTRegressionModel = GBTRegressionModel: uid=gbtr_d4d0e62c6394, numTrees=20, numFeatures=15\npredictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_18a22e86f3dd, metricName=rmse, throughOrigin=false\nrmse: Double = 57477.9022016309\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+--------+--------------------+\n        prediction|   label|            features|\n+------------------+--------+--------------------+\n 154172.2986563275| 94600.0|[-62.065401082150...|\n110296.36987270221|103600.0|[-62.040445150874...|\n144209.55530902446|106700.0|[-62.005506847089...|\n123196.33176988826|128900.0|[-61.975559729558...|\n 74594.01349042215| 68300.0|[-61.975559729558...|\n+------------------+--------+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 57477.9022016309\nimport org.apache.spark.ml.regression.GBTRegressor\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\ngb: org.apache.spark.ml.regression.GBTRegressor = gbtr_d4d0e62c6394\ngbModel: org.apache.spark.ml.regression.GBTRegressionModel = GBTRegressionModel: uid=gbtr_d4d0e62c6394, numTrees=20, numFeatures=15\npredictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_18a22e86f3dd, metricName=rmse, throughOrigin=false\nrmse: Double = 57477.9022016309\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["---\n# 6. Hyperparameter tuning\nAn important task in Machie Learning is model selection, or using data to find the best model or parameters for a given task. This is also called tuning. Tuning may be done for individual Estimators such as LinearRegression, or for entire Pipelines which include multiple algorithms, featurization, and other steps. Users can tune an entire Pipeline at once, rather than tuning each element in the Pipeline separately. MLlib supports model selection tools, such as `CrossValidator`. These tools require the following items:\n* Estimator: algorithm or Pipeline to tune (`setEstimator`)\n* Set of ParamMaps: parameters to choose from, sometimes called a \"parameter grid\" to search over (`setEstimatorParamMaps`)\n* Evaluator: metric to measure how well a fitted Model does on held-out test data (`setEvaluator`)\n\n`CrossValidator` begins by splitting the dataset into a set of folds, which are used as separate training and test datasets. For example with `k=3` folds, `CrossValidator` will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. To evaluate a particular `ParamMap`, `CrossValidator` computes the average evaluation metric for the 3 Models produced by fitting the Estimator on the 3 different (training, test) dataset pairs. After identifying the best `ParamMap`, `CrossValidator` finally re-fits the Estimator using the best ParamMap and the entire dataset.\n\nBelow, use the `CrossValidator` to select the best Random Forest model. To do so, you need to define a grid of parameters. Let's say we want to do the search among the different number of trees (1, 5, and 10), and different tree depth (5, 10, and 15)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e451c64f-0c9c-495c-98ee-ce025bdf76ba"}}},{"cell_type":"code","source":["%scala\ncvModel."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d07b3e28-8920-49bc-b8b5-46d2f41bf288"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res103: String =\nestimator: estimator for selection (current: linReg_2d8613e20454)\nestimatorParamMaps: param maps for the estimator (current: [Lorg.apache.spark.ml.param.ParamMap;@7d6c9ece)\nevaluator: evaluator used to select hyper-parameters that maximize the validated metric (current: RegressionEvaluator: uid=regEval_c994bc8ce01b, metricName=mse, throughOrigin=false)\nnumFolds: number of folds for cross validation (&gt;= 2) (default: 3, current: 3)\nseed: random seed (default: -1191137437)\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res103: String =\nestimator: estimator for selection (current: linReg_2d8613e20454)\nestimatorParamMaps: param maps for the estimator (current: [Lorg.apache.spark.ml.param.ParamMap;@7d6c9ece)\nevaluator: evaluator used to select hyper-parameters that maximize the validated metric (current: RegressionEvaluator: uid=regEval_c994bc8ce01b, metricName=mse, throughOrigin=false)\nnumFolds: number of folds for cross validation (&gt;= 2) (default: 3, current: 3)\nseed: random seed (default: -1191137437)\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n// TODO: Replace <FILL IN> with appropriate code\n\nimport org.apache.spark.ml.tuning.ParamGridBuilder\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.tuning.CrossValidator\n\nval paramGrid = new ParamGridBuilder()\n  .addGrid(rf.numTrees, Array(1, 5, 10))\n  .addGrid(rf.maxDepth, Array(5, 10 ,15))\n  .build()\n\nval evaluator = new RegressionEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"mse\")\nval cv = new CrossValidator()\n  .setEstimator(lr)\n  .setEvaluator(evaluator)\n  .setEstimatorParamMaps(paramGrid)\n  .setNumFolds(3)\nval cvModel = cv.fit(trainSet)\n\nval predictions = cvModel.transform(testSet)\npredictions.select(\"prediction\", \"label\", \"features\").show(5)\n\nval rmse = evaluator.evaluate(predictions)\nprintln(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e4b4782-8c60-4edf-9893-43cfc60363c0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"predictions","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"features","type":{"type":"udt","class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"type":"struct","fields":[{"name":"type","type":"byte","nullable":false,"metadata":{}},{"name":"size","type":"integer","nullable":true,"metadata":{}},{"name":"indices","type":{"type":"array","elementType":"integer","containsNull":false},"nullable":true,"metadata":{}},{"name":"values","type":{"type":"array","elementType":"double","containsNull":false},"nullable":true,"metadata":{}}]}},"nullable":true,"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"scaledFeatures_0"},{"idx":1,"name":"scaledFeatures_1"},{"idx":2,"name":"scaledFeatures_2"},{"idx":3,"name":"scaledFeatures_3"},{"idx":4,"name":"scaledFeatures_4"},{"idx":5,"name":"scaledFeatures_5"},{"idx":6,"name":"scaledFeatures_6"},{"idx":7,"name":"scaledFeatures_7"},{"idx":8,"name":"scaledFeatures_8"},{"idx":9,"name":"scaledFeatures_9"},{"idx":10,"name":"scaledFeatures_10"}],"binary":[{"idx":11,"name":"ocean_prox_encoded_<1H OCEAN"},{"idx":12,"name":"ocean_prox_encoded_INLAND"},{"idx":13,"name":"ocean_prox_encoded_NEAR OCEAN"},{"idx":14,"name":"ocean_prox_encoded_NEAR BAY"}]},"num_attrs":15}}},{"name":"label","type":"double","nullable":true,"metadata":{}},{"name":"prediction","type":"double","nullable":false,"metadata":{"ml_attr":{}}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+------------------+--------+--------------------+\n|        prediction|   label|            features|\n+------------------+--------+--------------------+\n|214144.81923173636| 94600.0|[-62.065401082150...|\n|181992.28784159466|103600.0|[-62.040445150874...|\n|222163.02506525663|106700.0|[-62.005506847089...|\n| 217628.7971001739|128900.0|[-61.975559729558...|\n| 152837.1718887746| 68300.0|[-61.975559729558...|\n+------------------+--------+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 5.003681282490371E9\nimport org.apache.spark.ml.tuning.ParamGridBuilder\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.tuning.CrossValidator\nparamGrid: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\n\trfr_f74e4657d0a5-maxDepth: 5,\n\trfr_f74e4657d0a5-numTrees: 1\n}, {\n\trfr_f74e4657d0a5-maxDepth: 5,\n\trfr_f74e4657d0a5-numTrees: 5\n}, {\n\trfr_f74e4657d0a5-maxDepth: 5,\n\trfr_f74e4657d0a5-numTrees: 10\n}, {\n\trfr_f74e4657d0a5-maxDepth: 10,\n\trfr_f74e4657d0a5-numTrees: 1\n}, {\n\trfr_f74e4657d0a5-maxDepth: 10,\n\trfr_f74e4657d0a5-numTrees: 5\n}, {\n\trfr_f74e4657d0a5-maxDepth: 10,\n\trfr_f74e4657d0a5-numTrees: 10\n}, {\n\trfr_f74e4657d0a5-maxDepth: 15,\n\trfr_f74e4657d0a5-numTrees: 1\n}, {\n\trfr_f74e4657d0a5-maxDepth: 15,\n\trfr_f74e4657d0a5-numTrees: 5\n}, {\n\trfr_f74e4657d0a5-maxDepth: 15,\n\trfr_f74e4657d0a5-numTrees: 10\n})\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_c994bc8ce01b, metricName=mse, throughOrigin=false\ncv: org.apache.spark.ml.tuning.CrossValidator = cv_e0d142d4c281\ncvModel: org.apache.spark.ml.tuning.CrossValidatorModel = CrossValidatorModel: uid=cv_e0d142d4c281, bestModel=LinearRegressionModel: uid=linReg_2d8613e20454, numFeatures=15, numFolds=3\npredictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\nrmse: Double = 5.003681282490371E9\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+--------+--------------------+\n        prediction|   label|            features|\n+------------------+--------+--------------------+\n214144.81923173636| 94600.0|[-62.065401082150...|\n181992.28784159466|103600.0|[-62.040445150874...|\n222163.02506525663|106700.0|[-62.005506847089...|\n 217628.7971001739|128900.0|[-61.975559729558...|\n 152837.1718887746| 68300.0|[-61.975559729558...|\n+------------------+--------+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 5.003681282490371E9\nimport org.apache.spark.ml.tuning.ParamGridBuilder\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.tuning.CrossValidator\nparamGrid: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\n\trfr_f74e4657d0a5-maxDepth: 5,\n\trfr_f74e4657d0a5-numTrees: 1\n}, {\n\trfr_f74e4657d0a5-maxDepth: 5,\n\trfr_f74e4657d0a5-numTrees: 5\n}, {\n\trfr_f74e4657d0a5-maxDepth: 5,\n\trfr_f74e4657d0a5-numTrees: 10\n}, {\n\trfr_f74e4657d0a5-maxDepth: 10,\n\trfr_f74e4657d0a5-numTrees: 1\n}, {\n\trfr_f74e4657d0a5-maxDepth: 10,\n\trfr_f74e4657d0a5-numTrees: 5\n}, {\n\trfr_f74e4657d0a5-maxDepth: 10,\n\trfr_f74e4657d0a5-numTrees: 10\n}, {\n\trfr_f74e4657d0a5-maxDepth: 15,\n\trfr_f74e4657d0a5-numTrees: 1\n}, {\n\trfr_f74e4657d0a5-maxDepth: 15,\n\trfr_f74e4657d0a5-numTrees: 5\n}, {\n\trfr_f74e4657d0a5-maxDepth: 15,\n\trfr_f74e4657d0a5-numTrees: 10\n})\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_c994bc8ce01b, metricName=mse, throughOrigin=false\ncv: org.apache.spark.ml.tuning.CrossValidator = cv_e0d142d4c281\ncvModel: org.apache.spark.ml.tuning.CrossValidatorModel = CrossValidatorModel: uid=cv_e0d142d4c281, bestModel=LinearRegressionModel: uid=linReg_2d8613e20454, numFeatures=15, numFolds=3\npredictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\nrmse: Double = 5.003681282490371E9\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["---\n# 7. An End-to-End Classification Test\nAs the last step, you are given a dataset called `data/ccdefault.csv`. The dataset represents default of credit card clients. It has 30,000 cases and 24 different attributes. More details about the dataset is available at `data/ccdefault.txt`. In this task you should make three models, compare their results and conclude the ideal solution. Here are the suggested steps:\n1. Load the data.\n2. Carry out some exploratory analyses (e.g., how various features and the target variable are distributed).\n3. Train a model to predict the target variable (risk of `default`).\n  - Employ three different models (logistic regression, decision tree, and random forest).\n  - Compare the models' performances (e.g., AUC).\n  - Defend your choice of best model (e.g., what are the strength and weaknesses of each of these models?).\n4. What more would you do with this data? Anything to help you devise a better solution?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4df1a628-c260-4f77-a7b6-532dd0d92966"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae6a1816-b84a-43df-a346-18e17966b00a"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"sparkml_lab","dashboards":[],"language":"python","widgets":{},"notebookOrigID":1972483464038750}},"nbformat":4,"nbformat_minor":0}
